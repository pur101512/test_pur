{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7xLvzwqxXlD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "import nltk\n",
        "import zipfile\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# ============================================================\n",
        "# 0. NLTK SETUP\n",
        "# ============================================================\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD INPUT FILE (SAFE LOADER)\n",
        "# ============================================================\n",
        "\n",
        "file_path = \"/home/sagemaker-user/CSAT/data/OCT_NEW_TRANSCRIPTS.xlsx\"\n",
        "\n",
        "def safe_read_input(file_path):\n",
        "    \"\"\"Reads file safely even if XLSX is corrupted.\"\"\"\n",
        "\n",
        "    # Case 1 â€” Check if XLSX is valid ZIP\n",
        "    if zipfile.is_zipfile(file_path):\n",
        "        print(\"âœ” Valid Excel file detected. Reading with openpyxlâ€¦\")\n",
        "        return pd.read_excel(file_path, engine=\"openpyxl\", dtype=str)\n",
        "\n",
        "    # Case 2 â€” Not a ZIP â†’ treat as CSV\n",
        "    print(\"âš  File is NOT a valid Excel. Trying CSV parsingâ€¦\")\n",
        "    return pd.read_csv(file_path, dtype=str, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
        "\n",
        "df = safe_read_input(file_path)\n",
        "df = df.copy()\n",
        "\n",
        "# Force ALL columns to string to prevent truncation\n",
        "df = df.astype(str)\n",
        "\n",
        "# If CX1 ID exists â†’ ensure no truncation\n",
        "for col in df.columns:\n",
        "    if \"cx1\" in col.lower() or \"call\" in col.lower() or \"id\" in col.lower():\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. REMOVE EMPTY TRANSCRIPTS\n",
        "# ============================================================\n",
        "NO_TEXT = [\"\", \"nan\", \"none\", \"n/a\", \"[]\"]\n",
        "\n",
        "summary_txt = df[\"Summary\"].astype(str).str.strip().str.lower()\n",
        "df = df[~summary_txt.isin(NO_TEXT)].copy()\n",
        "\n",
        "# ============================================================\n",
        "# 3. TIMESTAMP CLEANING\n",
        "# ============================================================\n",
        "def parse_ts(x):\n",
        "    try:\n",
        "        if isinstance(x, (int, float)) and float(x) > 30000:\n",
        "            return pd.to_datetime(\"1899-12-30\") + pd.to_timedelta(float(x), unit=\"D\")\n",
        "        return pd.to_datetime(x, errors=\"coerce\")\n",
        "    except:\n",
        "        return pd.NaT\n",
        "\n",
        "if \"timestamp\" in df.columns:\n",
        "    df[\"timestamp\"] = df[\"timestamp\"].apply(parse_ts)\n",
        "    df[\"timestamp\"] = df[\"timestamp\"].fillna(method=\"ffill\")\n",
        "\n",
        "    df[\"Date\"] = df[\"timestamp\"].dt.normalize()\n",
        "    df[\"YearMonth\"] = df[\"Date\"].dt.to_period(\"M\").astype(str)\n",
        "    df[\"Week\"] = df[\"Date\"].dt.to_period(\"W\").astype(str)\n",
        "\n",
        "    df[\"timestamp_str\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    df[\"Date_str\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. COMPLAINT DICT EXPANSION\n",
        "# ============================================================\n",
        "def parse_dict(val):\n",
        "    if pd.isna(val) or val == \"\":\n",
        "        return {}\n",
        "    try:\n",
        "        fixed = (\n",
        "            str(val)\n",
        "            .replace(\"true\", \"True\")\n",
        "            .replace(\"false\", \"False\")\n",
        "            .replace(\"null\", \"None\")\n",
        "        )\n",
        "        return ast.literal_eval(fixed)\n",
        "    except:\n",
        "        return {}\n",
        "\n",
        "if \"Complaint\" in df.columns:\n",
        "    df[\"Complaint_dict\"] = df[\"Complaint\"].apply(parse_dict)\n",
        "    complaints = pd.json_normalize(df[\"Complaint_dict\"]).add_prefix(\"Complaint_\")\n",
        "else:\n",
        "    complaints = pd.DataFrame(index=df.index)\n",
        "\n",
        "# ============================================================\n",
        "# 5. FLAGS EXPANSION\n",
        "# ============================================================\n",
        "if \"flags_list\" in df.columns:\n",
        "    df[\"flags_list\"] = df[\"flags_list\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "    df[\"flags_list\"] = df[\"flags_list\"].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "    max_flags = df[\"flags_list\"].apply(len).max()\n",
        "\n",
        "    flag_frames = []\n",
        "\n",
        "    for i in range(max_flags):\n",
        "        tmp = df[\"flags_list\"].apply(lambda lst: lst[i] if i < len(lst) else {})\n",
        "        tmp_df = pd.json_normalize(tmp).add_prefix(f\"Flag{i+1}_\")\n",
        "        flag_frames.append(tmp_df)\n",
        "\n",
        "    flags_expanded = pd.concat(flag_frames, axis=1) if max_flags > 0 else pd.DataFrame(index=df.index)\n",
        "else:\n",
        "    flags_expanded = pd.DataFrame(index=df.index)\n",
        "\n",
        "# ============================================================\n",
        "# 6. NLP CLEANING\n",
        "# ============================================================\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "extra_words = {\n",
        "    \"please\",\"kindly\",\"hello\",\"hi\",\"thank\",\"thanks\",\"sir\",\"madam\",\"customer\",\n",
        "    \"call\",\"agent\",\"support\",\"yeah\",\"ok\",\"okay\"\n",
        "}\n",
        "stop_words.update(extra_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_summary(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"Summary_Clean\"] = df[\"Summary\"].astype(str).apply(clean_summary)\n",
        "\n",
        "# ============================================================\n",
        "# 7. MERGE ALL FINAL COMPONENTS\n",
        "# ============================================================\n",
        "df_final = pd.concat(\n",
        "    [\n",
        "        df.drop(columns=[\"Complaint_dict\", \"flags_list\"], errors=\"ignore\"),\n",
        "        complaints,\n",
        "        flags_expanded\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 8. SAVE FINAL OUTPUT WITHOUT TRUNCATION (STRING SAFE)\n",
        "# ============================================================\n",
        "\n",
        "output_path = \"FINAL_TRANSCRIPTS_POWERBI_SAFE.xlsx\"\n",
        "\n",
        "# force all IDs to text\n",
        "df_final = df_final.astype(str)\n",
        "\n",
        "df_final.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
        "\n",
        "print(\"ðŸŽ‰ FINAL FILE CREATED SUCCESSFULLY â†’\", output_path)\n"
      ]
    }
  ]
}